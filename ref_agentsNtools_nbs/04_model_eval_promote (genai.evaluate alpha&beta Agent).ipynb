{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0d6398-81dc-4b5d-82c8-ab81031d3ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# UC Model Evaluation and Promotion (mlflow.genai.scorers)\n",
    "\n",
    "Evaluate latest registered UC model version using LLM scorers\n",
    "(RelevanceToQuery, Safety) and promote to 'champion' if metrics\n",
    "exceed threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e502e0b6-394d-4da8-add1-7fe87bba6612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required dependencies for MLflow model loading, including databricks-langchain and langchain\n",
    "%pip install -U -qqqq mlflow-skinny[databricks] databricks-langchain langchain psycopg[binary]\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d0efec-4102-46f9-980e-d1ea363d8b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\n",
    "    \"model_name\", \"mmt.LS_agent.lifesciences_agent\", \"Model Name (UC)\"\n",
    ")\n",
    "dbutils.widgets.text(\"catalog\", \"mmt\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"LS_agent\", \"Schema\")\n",
    "dbutils.widgets.text(\"promotion_threshold\", \"0.7\", \"Promotion Threshold\")\n",
    "dbutils.widgets.dropdown(\n",
    "    \"evaluation_metric\",\n",
    "    \"relevance\",\n",
    "    [\"relevance\", \"safety\", \"accuracy\"],\n",
    "    \"Evaluation Metric\",\n",
    ")\n",
    "\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "promotion_threshold = float(dbutils.widgets.get(\"promotion_threshold\"))\n",
    "evaluation_metric = dbutils.widgets.get(\"evaluation_metric\")\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Metric: {evaluation_metric}\")\n",
    "print(f\"Threshold: {promotion_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aab8aeb8-a4b4-4729-ae27-e94c19f0e4c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Model Signature?"
    }
   },
   "outputs": [],
   "source": [
    "# model_uri = f\"models:/{model_name}/{evaluator.get_latest_version()}\"\n",
    "# m = mlflow.pyfunc.load_model(model_uri)\n",
    "# print(m.metadata.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b6a120-bb89-4383-9d0c-d5bc15e018e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UPDATED Evaluator"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.genai import evaluate as genai_evaluate\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    version: str\n",
    "    metrics: Dict[str, float]\n",
    "    passed: bool\n",
    "    message: str\n",
    "\n",
    "\n",
    "class ModelEvaluator(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        catalog: str,\n",
    "        schema: str,\n",
    "        promotion_threshold: float = 0.7,\n",
    "        evaluation_metric: str = \"relevance\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.promotion_threshold = promotion_threshold\n",
    "        self.evaluation_metric = evaluation_metric\n",
    "        self.client = MlflowClient()\n",
    "        mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    def get_latest_version(self) -> str:\n",
    "        versions = self.client.search_model_versions(f\"name='{self.model_name}'\")\n",
    "        if not versions:\n",
    "            raise ValueError(f\"No versions found for {self.model_name}\")\n",
    "        return max(versions, key=lambda v: int(v.version)).version\n",
    "\n",
    "    def get_champion_version(self) -> Optional[str]:\n",
    "        try:\n",
    "            champion = self.client.get_model_version_by_alias(\n",
    "                self.model_name, \"champion\"\n",
    "            )\n",
    "            return champion.version\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_evaluation_dataset(self) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate_model(self, model_uri: str) -> Dict[str, float]:\n",
    "        pass\n",
    "\n",
    "    def promote_to_champion(self, version: str, metrics: Dict[str, float]) -> None:\n",
    "        metrics_str = \", \".join([f\"{k}={v:.3f}\" for k, v in metrics.items()])\n",
    "        comment = f\"Promoted by evaluation notebook. Metrics: {metrics_str}\"\n",
    "\n",
    "        self.client.set_registered_model_alias(self.model_name, \"champion\", version)\n",
    "\n",
    "        self.client.update_model_version(\n",
    "            name=self.model_name,\n",
    "            version=version,\n",
    "            description=f\"{comment}\\nPromoted at {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        )\n",
    "\n",
    "        print(f\"Promoted version {version} to champion\")\n",
    "\n",
    "    def run(self) -> EvaluationResult:\n",
    "        latest_version = self.get_latest_version()\n",
    "        print(f\"Evaluating version {latest_version}\")\n",
    "\n",
    "        model_uri = f\"models:/{self.model_name}/{latest_version}\"\n",
    "\n",
    "        metrics = self.evaluate_model(model_uri)\n",
    "        print(f\"\\nEvaluation metrics: {metrics}\")\n",
    "\n",
    "        primary_score = metrics.get(self.evaluation_metric, 0.0)\n",
    "        passed = primary_score >= self.promotion_threshold\n",
    "\n",
    "        if passed:\n",
    "            champion_version = self.get_champion_version()\n",
    "            if champion_version and champion_version == latest_version:\n",
    "                message = f\"Version {latest_version} is already champion\"\n",
    "            else:\n",
    "                self.promote_to_champion(latest_version, metrics)\n",
    "                message = f\"Version {latest_version} promoted to champion\"\n",
    "        else:\n",
    "            message = (\n",
    "                f\"Version {latest_version} did not meet threshold \"\n",
    "                f\"({primary_score:.3f} < {self.promotion_threshold})\"\n",
    "            )\n",
    "\n",
    "        return EvaluationResult(\n",
    "            version=latest_version, metrics=metrics, passed=passed, message=message\n",
    "        )\n",
    "\n",
    "\n",
    "class LifeSciencesEvaluator(ModelEvaluator):\n",
    "    \"\"\"Evaluate UC-registered Responses Agent model with mlflow.genai.evaluate.\"\"\"\n",
    "\n",
    "    def create_evaluation_dataset(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create simple evaluation dataset for mlflow.genai.evaluate.\n",
    "\n",
    "        Each sample has:\n",
    "        - 'inputs': dict, passed to predict_fn\n",
    "        - ict, used by scorers (e.g., expected_respons)\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\n",
    "                                f\"How many genes are in the \"\n",
    "                                f\"{self.catalog}.{self.schema}.genes_knowledge table?\"\n",
    "                            ),\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"genes_knowledge table\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"What is the average confidence for proteins?\",\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"average confidence proteins\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"user\", \"content\": \"Tell me about kinase proteins.\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"kinase proteins catalyze phosphorylation\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"user\", \"content\": \"What compounds target kinases?\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"compounds target kinases\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"user\", \"content\": \"Explain cell signaling pathways.\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"cell signaling pathways involve cascades\",\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "\n",
    "    def _predict_uc_model_raw(self, model_uri: str, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Call the UC Responses Agent model as its serving wrapper expects:\n",
    "        a dict with key 'input' containing the messages list.\n",
    "\n",
    "        Even though the underlying pyfunc signature shows input_json/output_json,\n",
    "        the wrapper hides that and directly accepts {'input': [...]}\n",
    "        and returns either:\n",
    "          - {'output': [...]}  (chat messages), or\n",
    "          - a JSON string / dict we normalize below.\n",
    "        \"\"\"\n",
    "        # inputs is like {\"input\": [ {...}, ... ]}\n",
    "        messages = inputs[\"input\"]\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "        # IMPORTANT: pass a dict, not a DataFrame\n",
    "        raw = model.predict({\"input\": messages})\n",
    "\n",
    "        # Normalize to something shaped like {'output': [...]}\n",
    "        # Cases:\n",
    "        #  1. raw is already {'output': [...]}\n",
    "        #  2. raw is a JSON string\n",
    "        #  3. raw is some other structure we wrap as text\n",
    "        if isinstance(raw, dict) and \"output\" in raw:\n",
    "            return raw\n",
    "\n",
    "        if isinstance(raw, str):\n",
    "            try:\n",
    "                parsed = json.loads(raw)\n",
    "                if isinstance(parsed, dict) and \"output\" in parsed:\n",
    "                    return parsed\n",
    "                elif isinstance(parsed, list):\n",
    "                    return {\"output\": parsed}\n",
    "                else:\n",
    "                    # treat as plain text\n",
    "                    return {\n",
    "                        \"output\": [\n",
    "                            {\n",
    "                                \"role\": \"assistant\",\n",
    "                                \"content\": [\n",
    "                                    {\"type\": \"output_text\", \"text\": str(parsed)}\n",
    "                                ],\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "            except Exception:\n",
    "                # not JSON, treat as plain text\n",
    "                return {\n",
    "                    \"output\": [\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"output_text\", \"text\": raw}\n",
    "                            ],\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "        # If it's a list, maybe it's already messages\n",
    "        if isinstance(raw, list):\n",
    "            return {\"output\": raw}\n",
    "\n",
    "        # Fallback: convert to text\n",
    "        return {\n",
    "            \"output\": [\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"output_text\", \"text\": str(raw)}\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def _extract_text_from_prediction(self, prediction: Any) -> str:\n",
    "        if isinstance(prediction, list) and prediction:\n",
    "            prediction = prediction[0]\n",
    "\n",
    "        if not isinstance(prediction, dict):\n",
    "            return \"\" if prediction is None else str(prediction)\n",
    "\n",
    "        output = prediction.get(\"output\")\n",
    "        if not isinstance(output, list) or not output:\n",
    "            return \"\"\n",
    "\n",
    "        # Look for assistant messages with structured content\n",
    "        for msg in reversed(output):\n",
    "            if (\n",
    "                isinstance(msg, dict)\n",
    "                and msg.get(\"role\") == \"assistant\"\n",
    "                and isinstance(msg.get(\"content\"), list)\n",
    "            ):\n",
    "                texts = [\n",
    "                    c.get(\"text\", \"\")\n",
    "                    for c in msg[\"content\"]\n",
    "                    if isinstance(c, dict)\n",
    "                    and c.get(\"type\") in (\"output_text\", \"text\")\n",
    "                ]\n",
    "                texts = [t for t in texts if t]\n",
    "                if texts:\n",
    "                    return \"\\n\".join(texts)\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def evaluate_model(self, model_uri: str) -> Dict[str, float]:\n",
    "        raw_dataset = self.create_evaluation_dataset()\n",
    "        print(f\"Running mlflow.genai.evaluate on {len(raw_dataset)} examples\")\n",
    "\n",
    "        # Outer \"inputs\" is for genai.evaluate; inner is passed to predict_fn_fn\n",
    "        eval_dataset = [\n",
    "            {\n",
    "                \"inputs\": {\"inputs\": row[\"inputs\"]},\n",
    "                \"expected_response\": row[\"expected_response\"],\n",
    "            }\n",
    "            for row in raw_dataset\n",
    "        ]\n",
    "\n",
    "        def predict_fn_fn(inputs: Dict[str, Any]) -> str:\n",
    "            # inputs is row[\"inputs\"], e.g. {\"input\": [ {role, content}, ... ]}\n",
    "            raw_pred = self._predict_uc_model_raw(model_uri, inputs)\n",
    "            return self._extract_text_from_prediction(raw_pred)\n",
    "\n",
    "        eval_result = genai_evaluate(\n",
    "            data=eval_dataset,\n",
    "            predict_fn=predict_fn_fn,\n",
    "            scorers=[RelevanceToQuery(), Safety()],\n",
    "        )\n",
    "\n",
    "        print(\"\\nRaw metrics from mlflow.genai.evaluate:\")\n",
    "        for k, v in eval_result.metrics.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "        relevance = float(eval_result.metrics.get(\"relevance_to_query/mean\", 0.0))\n",
    "        safety = float(eval_result.metrics.get(\"safety/mean\", 1.0))\n",
    "\n",
    "        metrics = {\n",
    "            \"relevance\": relevance,\n",
    "            \"accuracy\": relevance,\n",
    "            \"safety\": safety,\n",
    "        }\n",
    "\n",
    "        print(\"\\nMapped promotion metrics:\", metrics)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class LifeSciencesGenieEvaluator(LifeSciencesEvaluator):\n",
    "    def create_evaluation_dataset(self) -> List[Dict[str, Any]]:\n",
    "        base = super().create_evaluation_dataset()\n",
    "        genie = [\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"Compare average confidence scores between genes and proteins.\",\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"A comparison of average confidence scores between genes and proteins.\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"Show me top compounds with highest confidence scores.\",\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"A list or table of top compounds ordered by confidence score.\",\n",
    "            },\n",
    "        ]\n",
    "        return base + genie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb439f30-6bc5-466a-bdfe-4a431e2a5911",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluate with genai_evals"
    }
   },
   "outputs": [],
   "source": [
    "if \"genie\" in model_name.lower():\n",
    "    evaluator = LifeSciencesGenieEvaluator(\n",
    "        model_name=model_name,\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        promotion_threshold=0.7,\n",
    "        evaluation_metric=\"relevance\",\n",
    "    )\n",
    "else:\n",
    "    evaluator = LifeSciencesEvaluator(\n",
    "        model_name=model_name,\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        promotion_threshold=0.7,\n",
    "        evaluation_metric=\"relevance\",\n",
    "    )\n",
    "\n",
    "result = evaluator.run()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Result (UC ResponsesAgent + mlflow.genai)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Version: {result.version}\")\n",
    "print(f\"Passed: {result.passed}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric, score in result.metrics.items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "print(f\"\\n{result.message}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dd7ed55-9526-4857-8cb0-978ee449c42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc72a367-f1de-45a1-87b3-51ccfb529e51",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test OK Agent"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = LifeSciencesEvaluator(\n",
    "        model_name=\"mmt.LS_agent.LifeSciences_orchestrator_agent\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        promotion_threshold=0.7,\n",
    "        evaluation_metric=\"relevance\",\n",
    "    )\n",
    "\n",
    "result = evaluator.run()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Result (UC ResponsesAgent + mlflow.genai)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Version: {result.version}\")\n",
    "print(f\"Passed: {result.passed}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric, score in result.metrics.items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "print(f\"\\n{result.message}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e3a790-3070-4dd0-9839-9ae2814e3ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe009b91-b552-437b-9eec-fc2221dd5469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3681bd-49cb-49af-b1ae-9775900aa242",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test BAD Agent"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = LifeSciencesEvaluator(\n",
    "        model_name=\"mmt.LS_agent.orchestrator_bad_agent\",\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        promotion_threshold=0.7,\n",
    "        evaluation_metric=\"relevance\",\n",
    "    )\n",
    "\n",
    "result = evaluator.run()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Result (UC ResponsesAgent + mlflow.genai)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Version: {result.version}\")\n",
    "print(f\"Passed: {result.passed}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric, score in result.metrics.items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "print(f\"\\n{result.message}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202b297c-671f-46f9-8a0e-c900d00f060f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_model_eval_promote (genai.evaluate alpha&beta Agent)",
   "widgets": {
    "catalog": {
     "currentValue": "mmt",
     "nuid": "744ba813-a8eb-4838-9009-eefef59e7872",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mmt",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mmt",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "evaluation_metric": {
     "currentValue": "relevance",
     "nuid": "bb26e9ca-6512-4fa8-a53a-bf6c145a4200",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "relevance",
      "label": "Evaluation Metric",
      "name": "evaluation_metric",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "relevance",
        "safety",
        "accuracy"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "relevance",
      "label": "Evaluation Metric",
      "name": "evaluation_metric",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "relevance",
        "safety",
        "accuracy"
       ]
      }
     }
    },
    "model_name": {
     "currentValue": "mmt.LS_agent.lifesciences_agent",
     "nuid": "c6f28c70-de8b-44f0-be76-66bf90b4f4ac",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mmt.LS_agent.lifesciences_agent",
      "label": "Model Name (UC)",
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mmt.LS_agent.lifesciences_agent",
      "label": "Model Name (UC)",
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "promotion_threshold": {
     "currentValue": "0.7",
     "nuid": "51738ca9-2a45-4fd4-b123-f34992615deb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.7",
      "label": "Promotion Threshold",
      "name": "promotion_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.7",
      "label": "Promotion Threshold",
      "name": "promotion_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "LS_agent",
     "nuid": "02389da0-712d-48d3-be6f-2456f25a2edb",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "LS_agent",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "LS_agent",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
