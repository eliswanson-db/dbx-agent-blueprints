{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f46894-e8f5-47a2-bc91-f5a6c4aee38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### `alpha` Agent\n",
    "\n",
    "This code defines a Databricks LangGraph agent for a life‑sciences knowledge base that:\n",
    "Classifies the user’s question (with an “orchestrator” LLM node) into:\n",
    "\"sql\" → question is about counts/aggregations/statistics.\n",
    "\"vector\" → question needs semantic lookup in the knowledge base.\n",
    "If routed to sql:\n",
    "Uses Unity Catalog SQL functions (exposed as tools via UCFunctionToolkit) to:\n",
    "Count rows in genes/proteins/pathways/compounds knowledge tables.\n",
    "Count high‑confidence rows (given a min_confidence).\n",
    "Compute average confidence.\n",
    "Executes the tools via ToolNode, then calls the LLM again to produce a final natural language answer and returns it directly (no synthesizer step).\n",
    "If routed to vector:\n",
    "Runs two specialized vector workers in parallel using Send:\n",
    "Vector Worker 1 → searches genes + proteins vector indexes.\n",
    "Vector Worker 2 → searches pathways + compounds vector indexes.\n",
    "Each worker:\n",
    "Uses VectorSearchRetrieverTool tools bound to the LLM to issue vector‑search tool calls.\n",
    "Executes those calls via ToolNode.\n",
    "Collects raw search results.\n",
    "Calls the LLM again to summarize the results (with emphasis on key findings and confidence).\n",
    "Stores its own output plus metadata in state[\"worker_results\"] (e.g. result text, confidence, source, number of tool calls).\n",
    "Emits a status message like [Vector Worker X completed with N results].\n",
    "Synthesizer/judge node:\n",
    "Runs after both vector workers finish.\n",
    "Reads all worker_results, formats them into a markdown‑style summary, and builds a “meta‑prompt”:\n",
    "Explain the original question.\n",
    "Show each worker’s result and confidence.\n",
    "Asks the LLM to:\n",
    "Evaluate relevance & confidence.\n",
    "Reconcile any conflicts (favoring higher confidence).\n",
    "Produce a single coherent, scientific final answer, citing which worker results it used.\n",
    "Returns that synthesized answer as the final response.\n",
    "Integration / serving:\n",
    "Wraps the compiled LangGraph workflow into a custom LangGraphResponsesAgent that:\n",
    "Implements predict and predict_stream in MLflow’s ResponsesAgent interface.\n",
    "Converts inputs to OpenAI‑style chat format.\n",
    "Streams intermediate node messages and final deltas back to the caller.\n",
    "Registers this agent as the MLflow model (mlflow.models.set_model(AGENT)), with mlflow.langchain.autolog() enabled.\n",
    "In short:\n",
    "# It’s a multi‑worker, orchestrated agent that routes between SQL aggregations and parallel vector‑search workers over four life‑science knowledge bases, then synthesizes the vector workers’ findings into a single, high‑level scientific answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5e4076e-c6f0-4de4-961b-d46f12eaf5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53aae27b-f43c-4dc4-91a1-c1c629680e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### `beta` Agent\n",
    "\n",
    "# In summary: this agent is a multi-worker, UC- and vector-search-enabled orchestration graph, wrapped as an MLflow ResponsesAgent model. Architecturally it’s realistic (with routing, SQL tools, parallel retrievers, and a synthesizer), but the synthesizer is deliberately implemented to return a low-quality, generic answer.\n",
    "\n",
    "\n",
    "This agent is a Databricks-hosted, LangGraph-based orchestration agent that routes user questions to different “workers” (sub-agents) and returns an answer through the MLflow ResponsesAgent interface. At a high level it does the following:\n",
    "Takes chat-style input\n",
    "It expects a list of messages (user/assistant) as input, in OpenAI-style format, via MLflow’s ResponsesAgentRequest.\n",
    "It is wrapped as an MLflow pyfunc model using mlflow.models.set_model(AGENT), so you call it with mlflow.models.predict(...).\n",
    "Orchestrates between two main paths: SQL vs. Vector search\n",
    "The orchestrator_node looks at the last user message and decides:\n",
    "\"sql\" for questions about counts, averages, or aggregate statistics over the life sciences tables.\n",
    "\"vector\" for everything else that needs semantic search.\n",
    "The orchestrator returns a routing decision plus a small debug message like \"[Orchestrator: Routing to sql worker]\".\n",
    "Has a SQL worker that calls Unity Catalog functions\n",
    "The SQL worker (sql_worker_node) handles analytic questions such as:\n",
    "How many genes/proteins/pathways/compounds?\n",
    "How many “high confidence” entries above a threshold?\n",
    "What are average confidence scores?\n",
    "It uses:\n",
    "A UCFunctionToolkit with a set of Unity Catalog SQL functions (count_, count_high_confidence_, avg_confidence_*).\n",
    "A Databricks LLM (ChatDatabricks) bound to these tools, so the LLM decides which UC function(s) to call.\n",
    "Execution flow:\n",
    "LLM is prompted with the list of UC functions and the user’s question.\n",
    "If the LLM emits tool calls, a ToolNode executes those UC functions.\n",
    "The LLM is then invoked again with the original context plus the tool outputs to generate a natural-language answer.\n",
    "The node returns a final AIMessage containing that answer.\n",
    "Has two parallel vector search workers (multi-worker retrievers)\n",
    "If the orchestration route is \"vector\", it sends the state to:\n",
    "vector_worker_1_node (genes + proteins),\n",
    "vector_worker_2_node (pathways + compounds), in parallel using Send(...) from LangGraph.\n",
    "Each worker:\n",
    "Uses VectorSearchRetrieverTool configured to search the respective Unity Catalog vector search indexes for its domain.\n",
    "Prompts the LLM with a “you are Vector Worker X” instruction and the user question.\n",
    "Lets the LLM call the retriever tools to fetch semantically relevant entries.\n",
    "Optionally asks the LLM again to summarize the retrieved items.\n",
    "Stores a structured result in worker_results (with a “confidence” and a human-readable summary) and emits a debug message like \"[Vector Worker 1 completed with N results]\".\n",
    "Uses a synthesizer node – but here intentionally “bad”\n",
    "For vector questions, both vector workers feed into a synthesizer_node.\n",
    "In a normal design, the synthesizer would:\n",
    "Read all worker_results,\n",
    "Combine them,\n",
    "And produce a coherent, source-aware, high-quality answer.\n",
    "In this “BAD” version:\n",
    "The synthesizer ignores the worker results and the user’s question.\n",
    "It always returns a fixed, unhelpful answer:\n",
    "\"I don't know. I cannot provide a detailed answer to this question right now.\"\n",
    "Structurally, it’s correct (the graph and messaging shape stay compatible), but behaviorally it’s intentionally poor.\n",
    "Graph structure (LangGraph)\n",
    "Defined over AgentState with:\n",
    "messages (conversation history),\n",
    "worker_results (aggregated results from vector workers),\n",
    "route_decision (sql vs. vector).\n",
    "Nodes:\n",
    "orchestrator (entry point),\n",
    "sql_worker,\n",
    "vector_worker_1,\n",
    "vector_worker_2,\n",
    "synthesizer.\n",
    "Edges:\n",
    "orchestrator → conditionally:\n",
    "sql_worker then END, or\n",
    "both vector_worker_1 and vector_worker_2 in parallel.\n",
    "Both vector workers → synthesizer → END.\n",
    "MLflow ResponsesAgent wrapper\n",
    "LangGraphResponsesAgent adapts the LangGraph app to the MLflow ResponsesAgent protocol:\n",
    "predict_stream:\n",
    "Runs agent.stream(..., stream_mode=[\"updates\", \"messages\"]).\n",
    "For each node that emits messages, converts them into ResponsesAgentStreamEvent items via output_to_responses_items_stream.\n",
    "Also streams AIMessageChunk deltas as text deltas.\n",
    "predict:\n",
    "Consumes the stream, collects items where event.type == \"response.output_item.done\".\n",
    "Returns a ResponsesAgentResponse with output=[...] and any custom_outputs.\n",
    "This means the model returns a list of assistant messages (including debug/trace messages from orchestrator and workers), each with content chunks of type output_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e617e259-b68a-4024-80ce-d0102dd56edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "This code defines a Databricks LangGraph-based “orchestrator” agent for a life‑sciences knowledge base, then intentionally makes it behave badly at the final step.\n",
    "Below is what each major part does and how the whole thing behaves end‑to‑end.\n",
    "High‑level behavior\n",
    "Takes a user question (via ResponsesAgent / MLflow model).\n",
    "Orchestrator LLM decides a route:\n",
    "\"sql\" → use SQL/UC functions for counts and aggregates.\n",
    "\"vector\" → use vector search (semantic search) over genes/proteins/pathways/compounds.\n",
    "If sql:\n",
    "A SQL worker uses Databricks UC functions (UCFunctionToolkit) as tools to answer simple statistics questions and returns a direct answer.\n",
    "If vector:\n",
    "Two vector workers run (in parallel in graph terms):\n",
    "Vector Worker 1 → genes + proteins vector search tools.\n",
    "Vector Worker 2 → pathways + compounds vector search tools.\n",
    "Each worker:\n",
    "Calls the appropriate vector search tools.\n",
    "Gathers raw results.\n",
    "Asks the LLM to summarize them.\n",
    "Stores its own summary + metadata in worker_results.\n",
    "Then a synthesizer node is called to combine / judge worker outputs, but in this implementation it ignores everything and always returns the same bad answer:\n",
    "\"I don't know. I cannot provide a detailed answer to this question right now.\"\n",
    "So functionally:\n",
    "SQL questions → get a normal, data‑driven answer from the SQL worker.\n",
    "Semantic/vector questions → the two vector workers do all the right work, but the final answer the user sees is always that canned “I don’t know” message. The vector work is effectively thrown away.\n",
    "The structure is correct, but the agent is deliberately “broken” at the synthesizer step.\n",
    "Configuration\n",
    "ExamplePython\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "CATALOG = \"mmt\"\n",
    "SCHEMA = \"LS_agent\"\n",
    "VECTOR_SEARCH_ENDPOINT = \"ls_vs_mmt\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "Uses Databricks’ ChatDatabricks LLM endpoint (databricks-claude-3-7-sonnet).\n",
    "All UC functions and vector indexes are under mmt.LS_agent.*.\n",
    "Tools\n",
    "SQL / UC function tools\n",
    "Built from UCFunctionToolkit:\n",
    "ExamplePython\n",
    "UC_TOOL_NAMES = [\n",
    "    \"mmt.LS_agent.count_genes_knowledge_rows\",\n",
    "    \"mmt.LS_agent.count_proteins_knowledge_rows\",\n",
    "    ...\n",
    "    \"mmt.LS_agent.avg_confidence_compounds_knowledge\",\n",
    "]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
    "sql_tools = uc_toolkit.tools\n",
    "These are predefined UC scalar functions for:\n",
    "Counting rows per table (genes, proteins, pathways, compounds).\n",
    "Counting high‑confidence rows (param: min_confidence).\n",
    "Getting average confidence.\n",
    "Used only by the SQL worker.\n",
    "Vector search tools\n",
    "ExamplePython\n",
    "VECTOR_SEARCH_TOOLS = [\n",
    "    VectorSearchRetrieverTool(... name=\"search_genes\" ...),\n",
    "    VectorSearchRetrieverTool(... name=\"search_proteins\" ...),\n",
    "    VectorSearchRetrieverTool(... name=\"search_pathways\" ...),\n",
    "    VectorSearchRetrieverTool(... name=\"search_compounds\" ...),\n",
    "]\n",
    "Each tool:\n",
    "Uses a Unity Catalog vector search index, e.g.:\n",
    "mmt.LS_agent.genes_knowledge_vs_index\n",
    "mmt.LS_agent.proteins_knowledge_vs_index\n",
    "Returns top‑k (3) results with fields [\"id\", \"name\", \"description\", \"confidence\"].\n",
    "Used by Vector Worker 1 (genes, proteins) and Vector Worker 2 (pathways, compounds).\n",
    "Agent state\n",
    "ExamplePython\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "    worker_results: Annotated[Optional[dict[str, Any]], merge_worker_results]\n",
    "    route_decision: Optional[str]\n",
    "messages: chat history, merged with add_messages (LangGraph pattern).\n",
    "worker_results: dictionary merged via merge_worker_results so each worker can add its own result.\n",
    "route_decision: set by the orchestrator to \"sql\" or \"vector\".\n",
    "Orchestrator node\n",
    "ExamplePython\n",
    "def orchestrator_node(state: AgentState, config: RunnableConfig):\n",
    "    ...\n",
    "    orchestrator_prompt = \"\"\"You are an orchestrator for a life sciences knowledge base system.\n",
    "    ...\n",
    "    Respond with ONLY one word: either \"sql\" or \"vector\"\n",
    "    \"\"\"\n",
    "    response = llm.invoke(decision_messages)\n",
    "    decision = response.content.strip().lower()\n",
    "    ...\n",
    "    return {\n",
    "        \"route_decision\": decision,\n",
    "        \"messages\": [AIMessage(content=f\"[Orchestrator: Routing to {decision} worker]\")]\n",
    "    }\n",
    "Looks only at the last user message.\n",
    "Uses LLM to classify:\n",
    "Count/avg/statistics questions → \"sql\".\n",
    "Everything else → \"vector\".\n",
    "Writes a system‑style message indicating which worker is chosen.\n",
    "SQL worker node\n",
    "ExamplePython\n",
    "def sql_worker_node(state: AgentState, config: RunnableConfig):\n",
    "    ...\n",
    "    sql_llm = llm.bind_tools(sql_tools)\n",
    "    response = sql_llm.invoke(sql_messages)\n",
    "    ...\n",
    "    if response.tool_calls:\n",
    "        tool_node = ToolNode(sql_tools)\n",
    "        tool_results = tool_node.invoke({\"messages\": [response]})\n",
    "        ...\n",
    "        final_response = llm.invoke(sql_messages + result_messages)\n",
    "    ...\n",
    "    return {\"messages\": [AIMessage(content=final_answer)]}\n",
    "Flow:\n",
    "System prompt describes available UC functions and their role.\n",
    "LLM with sql_tools bound:\n",
    "Can produce tool calls like count_genes_knowledge_rows().\n",
    "ToolNode(sql_tools) actually executes those functions.\n",
    "LLM called again with:\n",
    "Original prompt + user + tool results\n",
    "To compose a natural language answer.\n",
    "Returns a single AIMessage with that answer.\n",
    "For SQL route, this is the final answer (graph edge goes to END).\n",
    "Vector worker 1: genes & proteins\n",
    "ExamplePython\n",
    "def vector_worker_1_node(state: AgentState, config: RunnableConfig):\n",
    "    worker_tools = [VECTOR_SEARCH_TOOLS[0], VECTOR_SEARCH_TOOLS[1]]\n",
    "    worker_llm = llm.bind_tools(worker_tools)\n",
    "    response = worker_llm.invoke(worker_messages)\n",
    "    ...\n",
    "    if response.tool_calls:\n",
    "        tool_node = ToolNode(worker_tools)\n",
    "        tool_results = tool_node.invoke({\"messages\": [response]})\n",
    "        ...\n",
    "        final_response = llm.invoke(\n",
    "            worker_messages\n",
    "            + result_messages\n",
    "            + [SystemMessage(content=\"Summarize the search results...\")]\n",
    "        )\n",
    "    ...\n",
    "    worker_results[\"vector_worker_1\"] = {\n",
    "        \"result\": final_result,\n",
    "        \"confidence\": 0.80,\n",
    "        \"source\": \"Vector Worker 1 (Genes & Proteins)\",\n",
    "        \"tool_calls_made\": len(response.tool_calls) if response.tool_calls else 0,\n",
    "    }\n",
    "    return {\n",
    "        \"worker_results\": worker_results,\n",
    "        \"messages\": [AIMessage(content=f\"[Vector Worker 1 completed with {len(search_results)} results]\")],\n",
    "    }\n",
    "Uses search_genes and search_proteins.\n",
    "Executes those tools via ToolNode.\n",
    "Extracts and concatenates raw search results.\n",
    "Calls LLM once more to create a summary of search results.\n",
    "Stores its final text + metadata in state[\"worker_results\"][\"vector_worker_1\"].\n",
    "Emits a status message like:\n",
    "[Vector Worker 1 completed with N results]\n",
    "Vector worker 2: pathways & compounds\n",
    "Same pattern, but with search_pathways and search_compounds and stores under \"vector_worker_2\".\n",
    "Synthesizer node (intentionally bad)\n",
    "ExamplePython\n",
    "def synthesizer_node(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"\n",
    "    BAD synthesizer: ignores worker_results and always answers poorly.\n",
    "    \"\"\"\n",
    "    bad_answer = (\n",
    "        \"I don't know. I cannot provide a detailed answer to this question right now.\"\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=bad_answer)]\n",
    "    }\n",
    "Ignores:\n",
    "state[\"worker_results\"]\n",
    "Vector worker summaries\n",
    "Any intermediate messages\n",
    "Always returns the same generic “I don’t know” answer.\n",
    "This is why the comment calls the agent “BAD behavior here”.\n",
    "In a proper implementation, this node would:\n",
    "Read state[\"worker_results\"].\n",
    "Compare confidence, tools used, content.\n",
    "Synthesize a final, helpful answer from the vector workers’ outputs.\n",
    "Routing logic and graph structure\n",
    "ExamplePython\n",
    "def route_after_orchestrator(state: AgentState):\n",
    "    decision = state.get(\"route_decision\", \"vector\")\n",
    "    if decision == \"sql\":\n",
    "        return Send(\"sql_worker\", state)\n",
    "    else:\n",
    "        return [\n",
    "            Send(\"vector_worker_1\", state),\n",
    "            Send(\"vector_worker_2\", state),\n",
    "        ]\n",
    "Graph wiring:\n",
    "ExamplePython\n",
    "workflow.add_node(\"orchestrator\", orchestrator_node)\n",
    "workflow.add_node(\"sql_worker\", sql_worker_node)\n",
    "workflow.add_node(\"vector_worker_1\", vector_worker_1_node)\n",
    "workflow.add_node(\"vector_worker_2\", vector_worker_2_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "workflow.set_entry_point(\"orchestrator\")\n",
    "workflow.add_conditional_edges(\"orchestrator\", route_after_orchestrator)\n",
    "workflow.add_edge(\"sql_worker\", END)\n",
    "workflow.add_edge(\"vector_worker_1\", \"synthesizer\")\n",
    "workflow.add_edge(\"vector_worker_2\", \"synthesizer\")\n",
    "workflow.add_edge(\"synthesizer\", END)\n",
    "orchestrator → either:\n",
    "sql_worker → END\n",
    "or\n",
    "vector_worker_1 & vector_worker_2 → synthesizer → END.\n",
    "Workers share state through worker_results (using merge_worker_results).\n",
    "MLflow / ResponsesAgent wrapper\n",
    "ExamplePython\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        ...\n",
    "    def predict_stream(self, request: ResponsesAgentRequest) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        ...\n",
    "Wraps the LangGraph agent into MLflow’s ResponsesAgent interface.\n",
    "predict_stream:\n",
    "Converts input to OpenAI‑style chat format with to_chat_completions_input.\n",
    "Calls agent.stream(..., stream_mode=[\"updates\", \"messages\"]).\n",
    "Streams back messages and deltas via ResponsesAgentStreamEvent.\n",
    "predict collects the final output items from the stream.\n",
    "Finally:\n",
    "ExamplePython\n",
    "mlflow.langchain.autolog()\n",
    "agent = create_orchestrator_agent()\n",
    "AGENT = LangGraphResponsesAgent(agent)\n",
    "mlflow.models.set_model(AGENT)\n",
    "Enables LangChain autologging.\n",
    "Creates the graph.\n",
    "Wraps it.\n",
    "Registers it as the MLflow model object.\n",
    "Summary in one sentence\n",
    "# This code implements a LangGraph‑based orchestrator–worker agent on Databricks that routes questions between SQL/UC tools and multiple vector‑search workers for a life‑sciences knowledge base, but intentionally uses a “broken” synthesizer that discards vector worker results and always replies with “I don’t know.”"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NOTES",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
