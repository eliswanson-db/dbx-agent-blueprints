{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0d6398-81dc-4b5d-82c8-ab81031d3ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# UC Model Evaluation and Promotion (`mlflow.genai.scorers`)\n",
    "\n",
    "Evaluate latest registered UC model version using LLM scorers\n",
    "(RelevanceToQuery, Safety) and promote to 'champion' if metrics\n",
    "exceed threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e502e0b6-394d-4da8-add1-7fe87bba6612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required dependencies for MLflow model loading, including databricks-langchain and langchain\n",
    "%pip install -U -qqqq mlflow-skinny[databricks] databricks-langchain langchain psycopg[binary]\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0d0118-9044-4a4c-8c88-b8e705fff230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Specify Configs"
    }
   },
   "outputs": [],
   "source": [
    "# Widgets for catalog, schema, and model base name\n",
    "\n",
    "dbutils.widgets.text(\"catalog\", \"mmt\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"LS_agent\", \"Schema\")\n",
    "dbutils.widgets.text(\"model_base_name\", \"lifesciences_agent\", \"Model Base Name\")\n",
    "\n",
    "dbutils.widgets.text(\"promotion_threshold\", \"0.7\", \"Promotion Threshold\")\n",
    "dbutils.widgets.dropdown(\n",
    "    \"evaluation_metric\",\n",
    "    \"relevance\",\n",
    "    [\"relevance\", \"safety\", \"accuracy\"],\n",
    "    \"Evaluation Metric\",\n",
    ")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "model_base_name = dbutils.widgets.get(\"model_base_name\")\n",
    "\n",
    "# Construct Fully Qualified Unity Catalog model name\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_base_name}\"\n",
    "\n",
    "print(f\"Using catalog: {catalog}, schema: {schema}\")\n",
    "print(f\"Model base name: {model_base_name}\")\n",
    "print(f\"FQ UC Model Name: {UC_MODEL_NAME}\")\n",
    "\n",
    "promotion_threshold = float(dbutils.widgets.get(\"promotion_threshold\"))\n",
    "evaluation_metric = dbutils.widgets.get(\"evaluation_metric\")\n",
    "\n",
    "print(f\"Threshold: {promotion_threshold}\")\n",
    "print(f\"Metric: {evaluation_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "aab8aeb8-a4b4-4729-ae27-e94c19f0e4c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Model Signature?"
    }
   },
   "outputs": [],
   "source": [
    "# model_uri = f\"models:/{model_name}/{evaluator.get_latest_version()}\"\n",
    "# m = mlflow.pyfunc.load_model(model_uri)\n",
    "# print(m.metadata.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b6a120-bb89-4383-9d0c-d5bc15e018e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluator"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.genai import evaluate as genai_evaluate\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    version: str\n",
    "    metrics: Dict[str, float]\n",
    "    passed: bool\n",
    "    message: str\n",
    "\n",
    "\n",
    "class ModelEvaluator(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        catalog: str,\n",
    "        schema: str,\n",
    "        promotion_threshold: float = 0.7,\n",
    "        evaluation_metric: str = \"relevance\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.promotion_threshold = promotion_threshold\n",
    "        self.evaluation_metric = evaluation_metric\n",
    "        self.client = MlflowClient()\n",
    "        mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    def get_latest_version(self) -> str:\n",
    "        versions = self.client.search_model_versions(f\"name='{self.model_name}'\")\n",
    "        if not versions:\n",
    "            raise ValueError(f\"No versions found for {self.model_name}\")\n",
    "        return max(versions, key=lambda v: int(v.version)).version\n",
    "\n",
    "    def get_champion_version(self) -> Optional[str]:\n",
    "        try:\n",
    "            champion = self.client.get_model_version_by_alias(\n",
    "                self.model_name, \"champion\"\n",
    "            )\n",
    "            return champion.version\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_evaluation_dataset(self) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate_model(self, model_uri: str) -> Dict[str, float]:\n",
    "        pass\n",
    "\n",
    "    def promote_to_champion(self, version: str, metrics: Dict[str, float]) -> None:\n",
    "        metrics_str = \", \".join([f\"{k}={v:.3f}\" for k, v in metrics.items()])\n",
    "        comment = f\"Promoted by evaluation notebook. Metrics: {metrics_str}\"\n",
    "\n",
    "        self.client.set_registered_model_alias(self.model_name, \"champion\", version)\n",
    "\n",
    "        self.client.update_model_version(\n",
    "            name=self.model_name,\n",
    "            version=version,\n",
    "            description=f\"{comment}\\nPromoted at {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        )\n",
    "\n",
    "        print(f\"Promoted version {version} to champion\")\n",
    "\n",
    "    def run(self) -> EvaluationResult:\n",
    "        latest_version = self.get_latest_version()\n",
    "        print(f\"Evaluating version {latest_version}\")\n",
    "\n",
    "        model_uri = f\"models:/{self.model_name}/{latest_version}\"\n",
    "\n",
    "        metrics = self.evaluate_model(model_uri)\n",
    "        print(f\"\\nEvaluation metrics: {metrics}\")\n",
    "\n",
    "        primary_score = metrics.get(self.evaluation_metric, 0.0)\n",
    "        passed = primary_score >= self.promotion_threshold\n",
    "\n",
    "        if passed:\n",
    "            champion_version = self.get_champion_version()\n",
    "            if champion_version and champion_version == latest_version:\n",
    "                message = f\"Version {latest_version} is already champion\"\n",
    "            else:\n",
    "                self.promote_to_champion(latest_version, metrics)\n",
    "                message = f\"Version {latest_version} promoted to champion\"\n",
    "        else:\n",
    "            message = (\n",
    "                f\"Version {latest_version} did not meet threshold \"\n",
    "                f\"({primary_score:.3f} < {self.promotion_threshold})\"\n",
    "            )\n",
    "\n",
    "        return EvaluationResult(\n",
    "            version=latest_version, metrics=metrics, passed=passed, message=message\n",
    "        )\n",
    "\n",
    "\n",
    "class LifeSciencesEvaluator(ModelEvaluator):\n",
    "    \"\"\"Evaluate UC-registered Responses Agent model with mlflow.genai.evaluate.\"\"\"\n",
    "\n",
    "    def create_evaluation_dataset(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create simple evaluation dataset for mlflow.genai.evaluate.\n",
    "\n",
    "        Each sample has:\n",
    "        - 'inputs': dict, passed to predict_fn\n",
    "        - ict, used by scorers (e.g., expected_respons)\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\n",
    "                                f\"How many genes are in the \"\n",
    "                                f\"{self.catalog}.{self.schema}.genes_knowledge table?\"\n",
    "                            ),\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"genes_knowledge table\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"What is the average confidence for proteins?\",\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"average confidence proteins\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"user\", \"content\": \"Tell me about kinase proteins.\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"kinase proteins catalyze phosphorylation\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"user\", \"content\": \"What compounds target kinases?\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"compounds target kinases\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"user\", \"content\": \"Explain cell signaling pathways.\"}\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"cell signaling pathways involve cascades\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def _predict_uc_model_raw(self, model_uri: str, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Call the UC Responses Agent model as its serving wrapper expects:\n",
    "        a dict with key 'input' containing the messages list.\n",
    "\n",
    "        Even though the underlying pyfunc signature shows input_json/output_json,\n",
    "        the wrapper hides that and directly accepts {'input': [...]}\n",
    "        and returns either:\n",
    "          - {'output': [...]}  (chat messages), or\n",
    "          - a JSON string / dict we normalize below.\n",
    "        \"\"\"\n",
    "        # inputs is like {\"input\": [ {...}, ... ]}\n",
    "        messages = inputs[\"input\"]\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "        # IMPORTANT: pass a dict, not a DataFrame\n",
    "        raw = model.predict({\"input\": messages})\n",
    "\n",
    "        # Normalize to something shaped like {'output': [...]}\n",
    "        # Cases:\n",
    "        #  1. raw is already {'output': [...]}\n",
    "        #  2. raw is a JSON string\n",
    "        #  3. raw is some other structure we wrap as text\n",
    "        if isinstance(raw, dict) and \"output\" in raw:\n",
    "            return raw\n",
    "\n",
    "        if isinstance(raw, str):\n",
    "            try:\n",
    "                parsed = json.loads(raw)\n",
    "                if isinstance(parsed, dict) and \"output\" in parsed:\n",
    "                    return parsed\n",
    "                elif isinstance(parsed, list):\n",
    "                    return {\"output\": parsed}\n",
    "                else:\n",
    "                    # treat as plain text\n",
    "                    return {\n",
    "                        \"output\": [\n",
    "                            {\n",
    "                                \"role\": \"assistant\",\n",
    "                                \"content\": [\n",
    "                                    {\"type\": \"output_text\", \"text\": str(parsed)}\n",
    "                                ],\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "            except Exception:\n",
    "                # not JSON, treat as plain text\n",
    "                return {\n",
    "                    \"output\": [\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"output_text\", \"text\": raw}\n",
    "                            ],\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "        # If it's a list, maybe it's already messages\n",
    "        if isinstance(raw, list):\n",
    "            return {\"output\": raw}\n",
    "\n",
    "        # Fallback: convert to text\n",
    "        return {\n",
    "            \"output\": [\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"output_text\", \"text\": str(raw)}\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def _extract_text_from_prediction(self, prediction: Any) -> str:\n",
    "        if isinstance(prediction, list) and prediction:\n",
    "            prediction = prediction[0]\n",
    "\n",
    "        if not isinstance(prediction, dict):\n",
    "            return \"\" if prediction is None else str(prediction)\n",
    "\n",
    "        output = prediction.get(\"output\")\n",
    "        if not isinstance(output, list) or not output:\n",
    "            return \"\"\n",
    "\n",
    "        # Look for assistant messages with structured content\n",
    "        for msg in reversed(output):\n",
    "            if (\n",
    "                isinstance(msg, dict)\n",
    "                and msg.get(\"role\") == \"assistant\"\n",
    "                and isinstance(msg.get(\"content\"), list)\n",
    "            ):\n",
    "                texts = [\n",
    "                    c.get(\"text\", \"\")\n",
    "                    for c in msg[\"content\"]\n",
    "                    if isinstance(c, dict)\n",
    "                    and c.get(\"type\") in (\"output_text\", \"text\")\n",
    "                ]\n",
    "                texts = [t for t in texts if t]\n",
    "                if texts:\n",
    "                    return \"\\n\".join(texts)\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def evaluate_model(self, model_uri: str) -> Dict[str, float]:\n",
    "        raw_dataset = self.create_evaluation_dataset()\n",
    "        print(f\"Running mlflow.genai.evaluate on {len(raw_dataset)} examples\")\n",
    "\n",
    "        # Outer \"inputs\" is for genai.evaluate; inner is passed to predict_fn_fn\n",
    "        eval_dataset = [\n",
    "            {\n",
    "                \"inputs\": {\"inputs\": row[\"inputs\"]},\n",
    "                \"expected_response\": row[\"expected_response\"],\n",
    "            }\n",
    "            for row in raw_dataset\n",
    "        ]\n",
    "\n",
    "        def predict_fn_fn(inputs: Dict[str, Any]) -> str:\n",
    "            # inputs is row[\"inputs\"], e.g. {\"input\": [ {role, content}, ... ]}\n",
    "            raw_pred = self._predict_uc_model_raw(model_uri, inputs)\n",
    "            return self._extract_text_from_prediction(raw_pred)\n",
    "\n",
    "        eval_result = genai_evaluate(\n",
    "            data=eval_dataset,\n",
    "            predict_fn=predict_fn_fn,\n",
    "            scorers=[RelevanceToQuery(), Safety()],\n",
    "        )\n",
    "\n",
    "        print(\"\\nRaw metrics from mlflow.genai.evaluate:\")\n",
    "        for k, v in eval_result.metrics.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "        relevance = float(eval_result.metrics.get(\"relevance_to_query/mean\", 0.0))\n",
    "        safety = float(eval_result.metrics.get(\"safety/mean\", 1.0))\n",
    "\n",
    "        metrics = {\n",
    "            \"relevance\": relevance,\n",
    "            \"accuracy\": relevance,\n",
    "            \"safety\": safety,\n",
    "        }\n",
    "\n",
    "        print(\"\\nMapped promotion metrics:\", metrics)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "## not actually using for the workflow example - theoretically works with the same approach\n",
    "class LifeSciencesGenieEvaluator(LifeSciencesEvaluator):\n",
    "    def create_evaluation_dataset(self) -> List[Dict[str, Any]]:\n",
    "        base = super().create_evaluation_dataset()\n",
    "        genie = [\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"Compare average confidence scores between genes and proteins.\",\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"A comparison of average confidence scores between genes and proteins.\",\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                    \"input\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \"Show me top compounds with highest confidence scores.\",\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"expected_response\": \"A list or table of top compounds ordered by confidence score.\",\n",
    "            },\n",
    "        ]\n",
    "        return base + genie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe009b91-b552-437b-9eec-fc2221dd5469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c31ebab-ae78-419f-bb89-fd0528c29ae9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluate"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = LifeSciencesEvaluator(\n",
    "        model_name=UC_MODEL_NAME,\n",
    "        catalog=catalog,\n",
    "        schema=schema,\n",
    "        promotion_threshold=promotion_threshold,\n",
    "        evaluation_metric=evaluation_metric,\n",
    "    )\n",
    "\n",
    "result = evaluator.run()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Result (UC ResponsesAgent + mlflow.genai)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Version: {result.version}\")\n",
    "print(f\"Passed: {result.passed}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric, score in result.metrics.items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "print(f\"\\n{result.message}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f9bda56-94bf-4b70-9815-8088d5362455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0873ef-136b-400a-8312-339040ae4859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "gather and set taskValues for downstream task"
    }
   },
   "outputs": [],
   "source": [
    "# Collect model evaluation and context variables from widgets or code outputs/definitions\n",
    "\n",
    "# Get from widgets if available\n",
    "try:\n",
    "    model_base_name = dbutils.widgets.get(\"model_base_name\")\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "    schema = dbutils.widgets.get(\"schema\")\n",
    "    UC_MODEL_NAME = f\"{catalog}.{schema}.{model_base_name}\"\n",
    "    promotion_threshold = float(dbutils.widgets.get(\"promotion_threshold\"))\n",
    "    evaluation_metric = dbutils.widgets.get(\"evaluation_metric\")\n",
    "except Exception:\n",
    "    # Fallbacks if widgets are not defined\n",
    "    model_base_name = globals().get(\"model_base_name\", \"\")\n",
    "    catalog = globals().get(\"catalog\", \"\")\n",
    "    schema = globals().get(\"schema\", \"\")\n",
    "    UC_MODEL_NAME = f\"{catalog}.{schema}.{model_base_name}\"\n",
    "    promotion_threshold = globals().get(\"promotion_threshold\", 0.7)\n",
    "    evaluation_metric = globals().get(\"evaluation_metric\", \"relevance\")\n",
    "\n",
    "# Get evaluation results from 'result' object if available\n",
    "try:\n",
    "    evaluated_version = result.version\n",
    "    evaluation_metrics = result.metrics\n",
    "    passed = result.passed\n",
    "    model_alias = 'champion' if passed else ''\n",
    "except Exception:\n",
    "    evaluated_version = \"\"\n",
    "    evaluation_metrics = {}\n",
    "    passed = False\n",
    "    model_alias = ''\n",
    "\n",
    "# Ensure evaluation_metrics is not empty \n",
    "if not evaluation_metrics:\n",
    "    raise ValueError(\"Evaluation metrics are empty! Please check the evaluation logic and dataset. The evaluation should return a non-empty metrics dictionary\")\n",
    "\n",
    "# Set TaskValues for downstream tasks using Databricks Jobs API | https://docs.databricks.com/aws/en/jobs/task-values\n",
    "\n",
    "dbutils.jobs.taskValues.set(\"model_base_name\", model_base_name)\n",
    "dbutils.jobs.taskValues.set(\"UC_MODEL_NAME\", UC_MODEL_NAME)\n",
    "dbutils.jobs.taskValues.set(\"catalog\", catalog)\n",
    "dbutils.jobs.taskValues.set(\"schema\", schema)\n",
    "dbutils.jobs.taskValues.set(\"promotion_threshold\", promotion_threshold)\n",
    "dbutils.jobs.taskValues.set(\"evaluation_metric\", evaluation_metric)\n",
    "dbutils.jobs.taskValues.set(\"evaluated_version\", evaluated_version)\n",
    "dbutils.jobs.taskValues.set(\"evaluation_metrics\", evaluation_metrics)\n",
    "dbutils.jobs.taskValues.set(\"passed\", passed)\n",
    "dbutils.jobs.taskValues.set(\"model_alias\", model_alias)\n",
    "\n",
    "# Print confirmation of what was set\n",
    "print(\"TaskValues set for downstream tasks:\")\n",
    "for key in [\n",
    "    \"model_base_name\", \"UC_MODEL_NAME\", \"catalog\", \"schema\", \"promotion_threshold\", \"evaluation_metric\", \n",
    "    \"evaluated_version\", \"evaluation_metrics\", \"passed\", \"model_alias\"\n",
    "]:\n",
    "    print(f\"{key}: {locals().get(key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a9e577c-f1cc-437b-ab01-897b4c63e971",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "downstream_task keyValues"
    }
   },
   "outputs": [],
   "source": [
    "# # Replace 'evaluate_model' with the actual task name of the upstream notebook in your job configuration\n",
    "# upstream_task = \"evaluate_model\"\n",
    "\n",
    "# model_base_name = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"model_base_name\", debugValue=\"lifesciences_agent\")\n",
    "# UC_MODEL_NAME = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"UC_MODEL_NAME\", debugValue=\"mmt.LS_agent.lifesciences_agent\")\n",
    "# catalog = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"catalog\", debugValue=\"mmt\")\n",
    "# schema = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"schema\", debugValue=\"LS_agent\")\n",
    "# promotion_threshold = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"promotion_threshold\", debugValue=0.7)\n",
    "# evaluation_metric = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"evaluation_metric\", debugValue=\"relevance\")\n",
    "# evaluated_version = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"evaluated_version\", debugValue=\"1\")\n",
    "# evaluation_metrics = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"evaluation_metrics\", \n",
    "#                                                  #debugValue={\"relevance\": 0.4, \"accuracy\": 0.4, \"safety\": 1.0} ## NA\n",
    "#                                                  debugValue={'relevance': 1.0, 'accuracy': 1.0, 'safety': 1.0} ## champion\n",
    "#                                                 )\n",
    "# passed = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"passed\", debugValue=True)\n",
    "# model_alias = dbutils.jobs.taskValues.get(taskKey=upstream_task, key=\"model_alias\", debugValue=\"champion\")\n",
    "\n",
    "# print(\"Retrieved TaskValues from upstream task:\")\n",
    "# for key in [\n",
    "#             \"model_base_name\", \"UC_MODEL_NAME\", \"catalog\", \"schema\", \"promotion_threshold\", \"evaluation_metric\", \n",
    "#             \"evaluated_version\", \"evaluation_metrics\", \"passed\", \"model_alias\"\n",
    "#            ]:\n",
    "#     print(f\"{key}: {locals().get(key)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_model_eval_promote (Workflow with taskValues specification)",
   "widgets": {
    "catalog": {
     "currentValue": "mmt",
     "nuid": "1a248661-d5cb-40b1-ae6d-c0bb9ff78ea0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mmt",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mmt",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "evaluation_metric": {
     "currentValue": "relevance",
     "nuid": "b9fa22b5-034e-4295-9bde-98fb7be4ff7b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "relevance",
      "label": "Evaluation Metric",
      "name": "evaluation_metric",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "relevance",
        "safety",
        "accuracy"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "relevance",
      "label": "Evaluation Metric",
      "name": "evaluation_metric",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "relevance",
        "safety",
        "accuracy"
       ]
      }
     }
    },
    "model_base_name": {
     "currentValue": "lifesciences_agent",
     "nuid": "7c6a149c-08e5-405d-be1a-c345b38c98d0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "lifesciences_agent",
      "label": "Model Base Name",
      "name": "model_base_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "lifesciences_agent",
      "label": "Model Base Name",
      "name": "model_base_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "promotion_threshold": {
     "currentValue": "0.7",
     "nuid": "8703cf2b-b895-458c-8ed8-23f68cf598de",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.7",
      "label": "Promotion Threshold",
      "name": "promotion_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.7",
      "label": "Promotion Threshold",
      "name": "promotion_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "LS_agent",
     "nuid": "b651220e-8564-491e-a7de-5841ed58bb18",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "LS_agent",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "LS_agent",
      "label": "Schema",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "vs_endpoint_name": {
     "currentValue": "ls_vs_mmt",
     "nuid": "64378485-3e0a-407b-b380-7780d013d5dc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ls_vs_mmt",
      "label": "VectorSearch_endpoint",
      "name": "vs_endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ls_vs_mmt",
      "label": "VectorSearch_endpoint",
      "name": "vs_endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
